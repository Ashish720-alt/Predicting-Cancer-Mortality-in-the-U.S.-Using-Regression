<h1><span style="color:#58a6ff;">ğŸ¯ Regression Project</span></h1>
<h3><span style="color:#d2a8ff;">Predicting Deaths Caused by Cancer</span></h3>

---

## âš™ï¸ <span style="color:#79c0ff;">Run</span>

```bash
python3 main.py
```

---

## ğŸ“Š <span style="color:#ffa657;">Possible Metrics</span>

### <span style="color:#a5d6ff;">Regression</span>

* ğŸ§® <span style="color:#c9d1d9;">RMSE</span>
* ğŸ“ <span style="color:#c9d1d9;">MAE</span>
* ğŸ“ˆ <span style="color:#c9d1d9;">RÂ²-score</span>

### <span style="color:#ffa657;">Classification</span>

* âœ… <span style="color:#c9d1d9;">Accuracy</span>
* ğŸ¯ <span style="color:#c9d1d9;">Precision</span>
* ğŸ” <span style="color:#c9d1d9;">Recall</span>
* âš–ï¸ <span style="color:#c9d1d9;">F1-score</span>
* ğŸ§  <span style="color:#c9d1d9;">AUC-ROC</span>

---

## ğŸ§© <span style="color:#56d364;">Pipeline Algorithm (`main.py`)</span>

### ğŸªœ <span style="color:#d2a8ff;">Step 1 â€” Split the Dataset</span>

| Split                                             | Purpose                                     | Portion |
| :------------------------------------------------ | :------------------------------------------ | :------ |
| ğŸ§  <span style="color:#79c0ff;">Training</span>   | Train the model                             | 60%     |
| ğŸ” <span style="color:#79c0ff;">Validation</span> | Tune hyperparameters / detect bias-variance | 20%     |
| ğŸ§¾ <span style="color:#79c0ff;">Test</span>       | Final evaluation                            | 20%     |

> ğŸ’¡ <span style="color:#8b949e;">Use stratified splitting for imbalanced classes or cross-validation for small datasets.</span>

---

### âš—ï¸ <span style="color:#d29922;">Step 2 â€” Baseline Model</span>

Start with simple, interpretable baselines:

* Regression â†’ <span style="color:#58a6ff;">Linear Regression</span> (`Î» = 0`, `degree = 1`)
* Classification â†’ <span style="color:#58a6ff;">Logistic Regression</span> or Decision Tree

Evaluate both:

* <span style="color:#79c0ff;">Training Error</span> â†’ fit quality
* <span style="color:#79c0ff;">Validation Error</span> â†’ generalization

> ğŸ¯ Establish this baseline before refinement.

---

### ğŸ”„ <span style="color:#ff7b72;">Step 3 â€” Cross-Validation for Hyperparameter Tuning</span>

Use **k-fold cross-validation** on training data.

Tune:

1. <span style="color:#d2a8ff;">Regularization strength Î»</span>
2. <span style="color:#d2a8ff;">Model complexity (degree)</span>

```text
1ï¸âƒ£  Split training set into k folds (k = 5)
2ï¸âƒ£  Train on kâˆ’1 folds, validate on 1 fold
3ï¸âƒ£  Compute average validation error
```

**Output:** best combination (e.g., Î» = 0.1, degree = 3).

---

### ğŸ§ª <span style="color:#a5d6ff;">Step 4 â€” Validate on Validation Set</span>

| Case                                                           | Symptom                                      | Fix                              |
| :------------------------------------------------------------- | :------------------------------------------- | :------------------------------- |
| âš ï¸ <span style="color:#ff7b72;">High Bias (Underfit)</span>    | High training & validation errors            | â†‘ degree / add features          |
| âš ï¸ <span style="color:#79c0ff;">High Variance (Overfit)</span> | Low training error but high validation error | â†‘ Î» / â†“ degree / remove features |

> ğŸ”§ *Minor Adjustments:*
>
> * Decrease Î» slightly â†’ bias fix
> * Increase Î» slightly â†’ variance fix
>   If no improvement â†’ Step 5.

---

### ğŸ§± <span style="color:#ff7b72;">Step 5 â€” Major Adjustments</span>

#### 5.1 ğŸ”§ Large Î» Changes

* High bias â†’ drastically lower Î»
* High variance â†’ drastically raise Î»
* Re-run cross-validation (Step 3 â†’ 4)

#### 5.2 ğŸ“‰ Optimize Training Set Size

If only variance high:
Plot training vs validation error over increasing data sizes.

* ğŸ§¨ *Large gap* â†’ add data (70 : 10 : 20)
* ğŸ”¹ *Small gap* â†’ move to feature optimization

#### 5.3 ğŸ§  Optimize Features

| Type          | Strategy                            |
| :------------ | :---------------------------------- |
| High Bias     | Add interaction/polynomial features |
| High Variance | Remove redundant features           |

Then redo Steps 3â€“4.

---

### ğŸ§¾ <span style="color:#56d364;">Step 6 â€” Evaluate on Test Set</span>

If failure â†’ report cause.
Else:

```text
Train on (Training + Validation)
Evaluate final performance on Test Set
```

---

## ğŸ§® <span style="color:#a5d6ff;">Notes â€” Linear Regression Variants</span>

### 1ï¸âƒ£ <span style="color:#79c0ff;">Ridge / Tikhonov Regression</span>

[
||\theta X âˆ’ y||_2^2 + ||M \theta||_2^2
]
If ( M = \alpha I ) â†’ standard L2 regularization.

> ğŸ§© Adds stability against multicollinearity.

---

### 2ï¸âƒ£ <span style="color:#ff7b72;">Lasso Regression</span>

[
||\theta X âˆ’ y||_2^2 + \alpha ||\theta||_1
]

> âš¡ Encourages sparse feature weights.

---

### 3ï¸âƒ£ <span style="color:#d2a8ff;">Polynomial Regression</span>

[
y = \sum_i Î²_i X'(x_i), \quad X'(x_i) = [1, x_i, x_i^2, â€¦, x_i^m]^T
]

> ğŸ¢ Nonlinear extension with univariate polynomial terms (no cross terms).
